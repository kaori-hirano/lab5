---
title: "Lab 5: Comparing Ridge, Lasso, and OLS"
author: "Kaori Hirano"
date: "06/07/2023"
format: pdf
---

# Packages

```{r load-packages}
# load packages here
library(ISLR2)
library(leaps) # for best subset selection
library(glmnet) # for ridge, LASSO, and elastic net
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(readr))
library(patchwork) # for plot arrangement
suppressPackageStartupMessages(library(pls)) # for principal component regression and partial least squares
```

# Data  

```{r load-data}
mls22 <- read_csv("data/mls22.csv", show_col_types = FALSE)
d <- mls22 |> drop_na()
```


# Data Wrangling

## Q1 

```{r chunk-name-here}
d <- d |>mutate(Nation, usa = if_else(Nation == "USA", 1, 0)) # 0 for non us, 1 for us

# 1 for gk, else 0
gkval <- grepl("GK", d$Pos, fixed = TRUE)
d <- d |> mutate(Pos, gk = if_else(gkval == TRUE, 1, 0))

# 1 for df, else 0
dfval <- grepl("DF", d$Pos, fixed = TRUE)
d <- d |> mutate(Pos, df = if_else(dfval == TRUE, 1, 0))
# head(d) works!

# 1 if mf, 0 if not 
mfval <- grepl("MF", d$Pos, fixed = TRUE)
d <- d |> mutate(Pos, mf = if_else(mfval == TRUE, 1, 0)) 

# 1 for FW and 0 for no
fwval <- grepl("FW", d$Pos, fixed = TRUE)
d <- d |> mutate(Pos, fw = if_else(fwval == TRUE, 1, 0))

d$logs <- log(d$base_salary)
```

# Comparing Predictive Performance

## Q2
```{r q2}
set.seed(16)
dim(d)
mls_cv <- data.frame(sample(727, (727*.7))) # creates cv set with 70 of data
class(mls_cv) 
head(mls_cv)
mls_test <- data.frame(sample(727, (727*.3))) # creates test set with other 30
```


## Q3

```{r predict-method}
predict.regsubsets <- function(object, newdata, id, ...) {
if(is.symbol(object$call[[2]])){
i <- 2
evals_form <- function(x){
!rlang::is_formula(eval(x), scoped = TRUE)
}
pos_evals_form <- possibly(evals_form, otherwise = FALSE)
while(pos_evals_form(object$call[[i]])){
i <- i + 1
}
tt <- eval(object$call[[i]])
} else {
tt <- as.formula(object$call[[2]])
}
mat <- model.matrix(tt, newdata)
coefj <- coef(object, id = id)
xvars <- names(coefj)
mat[, xvars] %*% coefj
}
```

Use best subsets with cross-validation (like in the example code for this week) to find the best
subset of variables from the ones use in the previous section. Plot the CV MSE against the size
of the model. For cross-validation use 10 folds; set the seed to 17 before doing fold assignment.
What does the optimum number of variables seem to be?
Then calculate the test set MSE (using mls22_test). You can do this using a for loop or
using a more efficient method.
Note 1: We have to do cross-validation by hand because there is no built-in cross-validation
function in the leaps package (unlike for ridge and Lasso). To do so, remember to first create
a vector called fold that assigns a fold ID to each observation in mls22_cv (you can use in
chunk cross-validation in this week’s example code as an example).
```{r q3-regfit}
# sets up model
set.seed(17)
train <- sample(c(TRUE, FALSE), nrow(d),
     replace = TRUE, prob=c(.7,.3))
test <- (!train)
model <- lm(logs ~ usa + gk + df+ mf+fw+Age+MP+Starts+Min+Gls+ Ast+PK+PKatt+CrdY+CrdR+xG+npxG+xAG+PrgC+PrgP+PrgR,
    data = d[train, ])

# gets prediction 
regfit_best <- regsubsets(logs ~ usa + gk + df+ mf+fw+Age+MP+Starts+Min+Gls+
                  Ast+PK+PKatt+CrdY+CrdR+xG+npxG+xAG+PrgC+PrgP+PrgR, d[train,], nvmax = 21)
reg_summary <- summary(regfit_best)
coef(regfit_best,9)
#set.seed(17)
mls_cv <- data.frame(sample(727, (727*.7))) # creates cv set with 70 of data
mls_test <- data.frame(sample(727, (727*.3))) # creates test set with other 30
#model <- lm(log(base_salary) ~ usa + gk + df+ mf+fw+Age+MP+Starts+Min+Gls+ Ast+PK+PKatt+CrdY+CrdR+xG+npxG+xAG+PrgC+PrgP+PrgR,    data = mls_cv)
#regfit_best <- predict.regsubsets(model, mls_cv, nvmax = 21)

#reg_summary <- summary(regfit_best)
#coef(regfit_best, 7)
```

```{r q3-cv}
# sets up cross validation
# number of folds
k <- 10
n <- nrow(d)
# for replicability
set.seed(17)
# assign observations to folds
folds <- sample(rep(1:k, length = n))
# create container for cross-validation error
cv_errors <- matrix(NA, k, 21,
    dimnames = list(NULL, paste(1:21)))
```

```{r cv-forloop}
# for loop for cross validation
for (j in 1:k) {
  best_fit <- regsubsets(logs ~ usa + gk + df+ mf+fw+Age+MP+Starts+Min+Gls+
                  Ast+PK+PKatt+CrdY+CrdR+xG+npxG+xAG+PrgC+PrgP+PrgR,
       data = d[folds != j, ],
       nvmax = 21)
  for (i in 1:21) {
    pred <- predict(best_fit, d[folds == j, ], id = i)
    cv_errors[j, i] <-
         mean((d$logs[folds == j] - pred)^2)
   }
 }
```

```{r plot-csv}
# plots the cv mse
mean_cv_errors <- apply(cv_errors, 2, mean)
mean_cv_errors

# plot translated to ggplot
data.frame(vars = 1:length(mean_cv_errors),
           mean_cv_errors = mean_cv_errors) %>% 
  ggplot(aes(x = vars, y = mean_cv_errors)) +
  geom_point() +
  geom_line() +
  labs(x = "Size of Model (# of Variables)",
       y = "Cross-Validation MSE") +
  theme_bw()
```

```{r refit-best}
reg_best <- regsubsets(logs ~ usa + gk + df+ mf+fw+Age+MP+Starts+Min+Gls+
                  Ast+PK+PKatt+CrdY+CrdR+xG+npxG+xAG+PrgC+PrgP+PrgR, data = d,
    nvmax = 21)
coef(reg_best, 9)
```

Based on this plot, the optimum number of variables seems to be 9 because this is where the mse is the lowest and seems to go up then taper off from there. These are being from the us, being defender, age, expected goals, being goalkeeper, games played, starts, xAG, and minutes played. 

```{r mse-q3}
# calculates mse for q3
val_mat <- model.matrix(model, data = d[val, ])

val_errors <- rep(NA, 21)
for (i in 1:21) {
 coefi <- coef(regfit_best, id = i)
 pred <- val_mat[, names(coefi)] %*% coefi
 val_errors[i] <- mean((d$logs[val] - pred)^2)
}

mean(val_errors)
which.min(val_errors)
coef(regfit_best, 16)

# gets mse using function
get_mse <- function(i){
 coefi <- coef(regfit_best, id = i)
 pred <- val_mat[, names(coefi)] %*% coefi
 val_errors[i] <- mean((d$logs[val] - pred)^2)
}

val_errors_purrr <- map_dbl(1:21,
                            get_mse)

#cbind(val_errors_purrr, val_errors)
which.min(val_errors_purrr)
coef(regfit_best, 14) # here it says the best is 14, which is 
# about the same visually as 9 so that makes sense

# prints mse
mean(val_errors_purrr)
```
From the test set, the MSE is .6202 


## Q4
Use cv.glmnet() to do 10-fold cross-validation to find the optimal 𝜆 for ridge regression. Set
the seed to 18 beforehand. Use a 𝜆 sequence (check the help file for cv.glmnet() to figure
out which argument to use) of 10^seq(10, -2, length = 100). Remember that you need
to create an x matrix and y vector for cv.glmnet(), like in the example code.
What is the optimal 𝜆? What is the test set MSE?
```{r q4}
set.seed(18)

# sets up matrix
x <- model.matrix(logs ~ usa + gk + df+ mf+fw+Age+MP+Starts+Min+Gls+ Ast+PK+PKatt+CrdY+CrdR+xG+npxG+xAG+PrgC+PrgP+PrgR, d)[, -1]
y <- d$logs

# fits ridge 
grid <- 10^seq(10, -2, length = 100)
ridge_mod <- cv.glmnet(x, y, alpha = 0, lambda = grid)

train <- sample(1:nrow(x), nrow(x) / 2)
val <- (-train)
y_val <- y[val]

# fit ridge on train
ridge_mod2 <- glmnet(x[train, ], y[train], alpha = 0,
    lambda = grid, thresh = 1e-12)
ridge_pred <- predict(ridge_mod2, s = 4, newx = x[val, ])

# get best l
cv_out <- cv.glmnet(x[train, ], y[train], alpha = 0)

bestlam <- cv_out$lambda.min
bestlam

# get mse
ridge_pred <- predict(ridge_mod, s = bestlam,
    newx = x[val, ])
mean((ridge_pred - y_val)^2)
```
The best lam is .0533. The test set MSE is .5834.

## Q5
Use cv.glmnet() to do cross-validation to find the optimal 𝜆 for Lasso regression. Set the
seed to 18 beforehand. Use all the same parameter as in Q4.
What is the optimal 𝜆? What is the test set MSE? Which of the three approaches has the
lowest test MSE
```{r cv-lasso}
set.seed(18)
lasso_mod <- glmnet(x[train, ], y[train], alpha = 1,
    lambda = grid)

# get l
cv_out_lasso <- cv.glmnet(x[train, ], y[train], alpha = 1)
bestlaml <- cv_out_lasso$lambda.min
bestlaml

# gets MSe
lasso_pred <- predict(lasso_mod, s = bestlaml,
    newx = x[val, ])
mean((lasso_pred - y_val)^2)
```
The optimal l is .0010. The test set MSE is .6344. The approach with the lowest test MSE is from ridge regression.  

# Comparing Variable Selection Approaches

## Q6
Based on the optimal Lasso model, which player characteristics seem to be important for
predicting 2022 MLS player salaries? Remember to refit the model on the whole data with
the optimal 𝜆 found in the previous section.
```{r q6}
# refits to full data, uses best l to get coefs
out <- glmnet(x, y, alpha = 1, lambda = grid)
lasso_coef <- predict(out, type = 'coefficient', s = bestlaml)
lasso_coef
```
The characteristics that seem to be important with the lasso model are being from the us, being a goalkeeper, defender, or forward, as well as age, num games played, minutes played (but barely), penalty kicks attempted (barely), yellow cards, red cards, expected goals, nonpenalty expected goals, and progressive passes by the player. 

## Q7
Based on the optimal best subsets model, which player characteristics seem to be important
for predicting 2022 MLS player salaries? Remember to refit the model on the whole data with
the optimal number of variables found in the previous section.
```{r q3}
# gets best subsets for 9, which was what the determined best number was
reg_best_full <- regsubsets(logs ~ usa + gk + df+ mf+fw+Age+MP+Starts+Min+Gls+
                  Ast+PK+PKatt+CrdY+CrdR+xG+npxG+xAG+PrgC+PrgP+PrgR, data = d,
    nvmax = 9)

# gets full best subsets to check
reg_best_full1 <- regsubsets(logs ~ usa + gk + df+ mf+fw+Age+MP+Starts+Min+Gls+
                  Ast+PK+PKatt+CrdY+CrdR+xG+npxG+xAG+PrgC+PrgP+PrgR, data = d,
    nvmax = 21)

# checks that they are same and displays important characteristics
cbind(coef(reg_best_full1, 9), coef(reg_best_full, 9))

```
The characteristics that seem most important are being from the us, being goalkeeper, being
defender, age, games played, minutes played, starts, expected non-penalty and overall goals. 

## Q8
Compare the variables selected by Lasso and by best subsets approach. Are they the same?
Different? Does this match what you would expect, based on what you know about soccer
and salaries (which doesn’t have to be a lot, don’t worry!).

They are mostly the same, with lasso having a few more important characteristics. I think the best subsets
was mostly what I would think of in general with my limited soccer knowledge, like age and expected goals, 
while the lasso got into more details that might be something that 
someone who knows more about soccer may expect to influence pay, such as yellow cards and red cards. Overall, what I expected to be included was mostly included and the two models have overlap in what is selected. 
